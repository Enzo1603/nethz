# SEO-Konfiguration

## √úbersicht

Die Website ist f√ºr Google-Indexierung optimiert mit:
- **robots.txt**: Dynamisch generiert, blockiert nur private Bereiche
- **sitemap.xml**: Automatisch generiert, enth√§lt alle Seiten in beiden Sprachen (de + en)
- **Zweisprachig**: Jede Seite wird f√ºr Deutsch und Englisch indexiert

## Wie es funktioniert

### robots.txt (`/robots.txt`)
- Wird dynamisch durch `nethz_django/views.py` generiert
- Enth√§lt korrekte Domain und Sitemap-URL
- Blockiert: `/admin/`, `/de/accounts/`, `/en/accounts/`
- Erlaubt: Alle anderen wichtigen Seiten

### sitemap.xml (`/sitemap.xml`)
- Wird durch `nethz_django/sitemaps.py` generiert
- **44 URLs total** (22 Deutsch + 22 Englisch)
- Beinhaltet:
  - Statische Seiten (Home, Worldle, etc.)
  - TM Semester (automatisch aus Datenbank)
  - Worldle Regionen (automatisch aus Code)

### Automatische Updates
- **TM Semester**: Neue Semester mit `TM_` Prefix werden automatisch zur Sitemap hinzugef√ºgt
- **Worldle Regionen**: Werden aus `worldle/country_data.py` gelesen
- **Keine manuelle Wartung n√∂tig**

## Deployment

```bash
# 1. Code deployen
git push

# 2. Auf Server aktualisieren
ssh server
cd /path/to/nethz
git pull
source .venv/bin/activate
sudo systemctl restart gunicorn

# 3. Validieren
curl https://nethz.baraldi.ch/robots.txt
curl https://nethz.baraldi.ch/sitemap.xml | head -50
```

## Google Search Console

1. **Sitemap einreichen**:
   - Gehe zu: https://search.google.com/search-console
   - Sitemaps ‚Üí Neue Sitemap hinzuf√ºgen
   - URL: `https://nethz.baraldi.ch/sitemap.xml`

2. **robots.txt testen**:
   - Einstellungen ‚Üí robots.txt-Tester
   - Pr√ºfen dass wichtige URLs erlaubt sind

3. **Coverage √ºberwachen**:
   - Nach 2-4 Wochen sollte "Indexiert" steigen
   - "Gecrawlt aber nicht indexiert" sollte sinken

## Erwartungen

- **Woche 1-2**: Google crawlt die Sitemap
- **Woche 2-4**: Erste URLs werden indexiert (~20-30)
- **Monat 2-3**: Vollst√§ndige Indexierung (~40-44 URLs)

## Dateien

### Ge√§ndert/Neu:
- `nethz_django/views.py` - robots.txt View
- `nethz_django/sitemaps.py` - i18n-Unterst√ºtzung mit `I18nSitemap`
- `nethz_django/urls.py` - Verwendet robots.txt View

### Kann gel√∂scht werden:
- `templates/robots.txt` - Nicht mehr verwendet

## Troubleshooting

### URLs testen
```bash
# Lokal
python manage.py runserver
curl http://localhost:8000/robots.txt
curl http://localhost:8000/sitemap.xml

# Production
curl https://nethz.baraldi.ch/robots.txt
curl https://nethz.baraldi.ch/sitemap.xml
```

### H√§ufige Probleme
- **"Gecrawlt aber nicht indexiert"**: Normal bei neuen Websites, 2-4 Wochen warten
- **"Durch robots.txt blockiert"**: Nur `/admin/` und `/*/accounts/` sollten blockiert sein
- **Fehlende URLs**: Django Check durchf√ºhren: `python manage.py check`

## Neue Inhalte hinzuf√ºgen

### Neues TM Semester (automatisch)
```python
ExerciseSession.objects.create(
    short_name="TM_FS25",
    name="Technische Mechanik FS25"
)
# ‚Üí Erscheint automatisch in Sitemap!
```

### Neue statische Seite (manuell)
In `nethz_django/sitemaps.py` ‚Üí `StaticViewSitemap.items()`:
```python
return [
    "main:home",
    "main:neue_seite",  # Hinzuf√ºgen
    # ...
]
```

Das war's! Simple und wartungsfrei. üöÄ
